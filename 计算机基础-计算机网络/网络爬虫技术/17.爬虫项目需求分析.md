#### 网络爬虫
### 基本概述
- 网络爬虫是一种**自动化的万维网资源下载器**(也被称为网络蜘蛛，网络机器人)，
- 可以自动批量下载网络资源（图片、数据）,是**获取数据资源的一种高效工具**
- 是一种基于**Web的网络应用**;是**数据分析**,**数据可视化**,**数据挖掘**,AI(人工智能),大数据等领域的基础;
- 爬虫为搜索引擎提供数据采集功能。（百度，Google）
- 将人访问资源的过程封装成任务;交给爬虫自动完成;极大提升了工作效率(快,数据量大,自动运行)
- 爬虫程序只能爬取数据,但数据分析比爬虫工作更加复杂;

#### 爬虫的工作范围---万维网
- 万维网（以www.作为开头的所有的网页服务），即万维网服务，
- 万维网是一种获取外部数据资源的途径，
- 用户通过浏览器访问万维网，（爬虫工作于万维网）
- 浏览器是主要获取信息资源的渠道

#### 网络中的数据资源类型：
文本资源：      txt     html     xml    shtml
图像资源：      jpg     gif      bmp    png
音频资源：      mp3
视频资源：      mp4     rvmb    flv     avi..

###  爬虫的工作流程（网络资源下载过程+一些修改）
 1. 模拟浏览器行为
    -  确定种子页面的URL
    -  解析URL:
       -  获取主机IP地址\端口号\资源文件名
       -  
    -  模拟认证
    -  安全传输认证
    -  超文本传输协议
    -  完成请求响应
    -  数据解析


 2. 访问特定资源网站
    - 设定一个爬虫种子页面 
    - 
    - 完成与Web服务器的网络连接
    - 发送访问请求
    - 获取响应内容
 3. 下载资源;
 4. 拓扑出其他网页链接;
    - 网站之间的关联性:网页中存有较多可以跳转到其他网页的链接
    - 爬虫通过网页中大量的出链接和入链接来访问其他网页;
 5. 重复爬虫工作直至终止条件;


#### URL的概念:统一资源定位符（万维网中资源的地址）
1. URL的组成:
   1. http协议版本：**http**  **https** 两种协议
   2. 主机名（域名）：可视为一台电脑
   3. 资源所在路径（层级目录）：为实际文件存储在主机的哪个位置
   4. 资源名（文件名）：目标文件的名称
2. URL的格式:
   
         http协议版本 ://      域名     /    存储路径    / 资源文件名(带后缀)
         http        :// news.sina.net /    tmp/csa    / 20200401.shtml

#### http协议:超文本传输协议（基于TCP实现）,用于万维网中数据交互
  1. http协议 与 https协议的区别:http secure协议
      - http协议:传输交互数据,不进行加密;连接易于被拦截;
      - https协议:https协议在于TCP协议的接口处使用了一个SSL接口使得数据交互过程中都进行了数据加密与认证解密,极大地提升了安全性;
   - http的请求和响应
   - 下载器向Web服务器发送请求
   - 服务器向下载器发送响应消息
### 网络资源下载过程
1. 准备种子URL资源地址

         http://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word=%E5%9B%BE%E7%89%87

         http://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word=%E5%9B%BE%E7%89%87

2. 配置虚拟机网络：本机设置NET模式，与Windows共用同一网卡

3. 解析服务器地址：通过 URL解析出服务器IP和PORT

4. 与Web服务器主机建立连接

### 网络资源下载器的实现

         









### http网络协议的交互过程//http协议基于TCP协议;需要先封装一个数据包()
1. 请求头的组成:(固定格式与字段的长字符串)
   - 请求方式+请求资源地址+指定HTTP协议的版本+指定接收的数据类型及权重系数+浏览器版本和兼容说明+所访问的主机名+链接方式
   1. 请求方式:
        - GET方式
        - POST方式
   2. 请求资源地址(URL)
   3. 指定HTTP协议的版本
   4. 指定接收的数据类型及权重系数(Accept优先级q)
      1. 用于区分所接受的数据类型
      2. 权重系数越高,优先级越高??
   5. 浏览器版本和兼容说明(自定义User - Agent)   Web服务器所接收对象的角色
   6. 所访问的主机名(服务器域名Host)
   7. 链接方式:
      - close:短链接,一次性链接,客户端程序与服务器交互一次后,服务器立即断开;
      - keep-alive:长连接,浏览器与服务器保持连接持续交互;
   8. 注意:
      - \r\n 是HTML语言中的换行符
2. 返回的响应数据()服务器所返回的数据
   - 响应头+响应体(程序应获取判断响应头(根据状态码判断),而保存响应体数据)
   1. 响应头:指定格式指定字段的长字符串
      1. 服务器名
      2. 版本号
      3. 服务器缓存信息缓存位置
      4. 服务器缓存时间和响应时间
      5. 服务器响应状态码(Status Code)
         - 200 OK 服务器响应资源成功
         - 301,302 资源重定向
         - 400
         - 403
         - 404 :失败
         - 501 :服务器异常
         - 502
      6. 服务器响应资源大小
   2. 响应体:服务器所响应的数据资源



### 爬虫的拓扑工作流程
1. .
2. URL管理分析与实现
   1. 一个链接的传递过程,可以很好分析出新的链接并对其进行处理和下载(拓扑中获得新连接的过程)
   2. URL链接的处理原则:
      - 每一个链接必须仅能处理一次,重复出现的链接必须进行去重
      - 两容器中必须都进行去重工作,保证URL唯一;
      - 去重工作在将链接加入未处理容器中之前;
      1. 具体流程:
         1. 确定种子URL
         2. 和两容器中所有链接比较去重
         3. 将去重后的新链接加入未处理URL的连接容器
            1. (不可知,未下载的链接)
         4. 将连接被持久化器获取已处理完成的URL:
            1. 分析URL
            2. 链接WEB
            3. 请求响应
            4. 持久化文件
         5. 将连接加入已处理过的URL链接容器
            1. 可知,已下载的链接,
            2. 是去重工作的重要凭证;
            3. 链接数只增不减:MD5,MD6压缩算法压缩链接大小;
         6. 判断是否结束爬取数据
            -  继续爬取则将连接复制出资源文件解析器以获取更多的链接(拓扑)将新连接作为新的种子链接重复上述操作;
         7. 注意:
            1. 解析频率高会导致未处理容器的访问频率远大于已处理容器,应对解析频率加以限制;














































相关文件目录
1）include/头文件目录		2）src/源码目录	3）lib/库目录		4）Makefile/项目管理

头文件Download.h中
1）包含基本头文件
stdio.h		stdlib.h		unistd.h		string.h
2）url_t 		设定一个记录url信息的结构体
其中包含结构体成员：
1）原始url地址		2）协议类型		3）主机域名		4）资源路径
5）资源名/文件名		6）IP地址		7）端口号	(Web 服务器默认端口：http: 80,	https:443)
包含函数模块声明：
int Analytic_Url（url_t *） 	URL连接解析函数模块
int Socket_Create(void)		socket的创建，绑定
int Connect_Webserver(int Socket,url_t *)	链接Web服务器




gethostbyname（）


测试用主控接口












响应头字段：
1）status：响应状态码
2）length：资源大小
响应状态码尤为重要：得到响应头 获取响应码 处理响应体
四种极其常用的响应状态码
200	OK	：	表示服务器成功响应
301	302	：	表示资源重定向（资源确实在这个URL中，但地址易发生改变）
404		：	资源不存在（资源已不存在与对应服务器）
501	502	：	服务器中断，服务器异常

获取响应码：根据响应码决定是否继续处理
-->请求端对响应码进行处理
-->得到持久化资源（资源数据）













爬虫持续工作的原理
step1：第一次去重
种子URL在进入待处理队列之前
先通过URL已处理集合进行去重操作
如果已处理队列中存在和种子URL中重复的
在种子URL中删除（）
step2：第二次去重
种子URL在进入待处理队列之前
进行与未处理队列的去重操作
（保证未处理队列的唯一性）



















连接网络函数模块:

      int Cspider_Connect_Webserver(url_t *u,int webfd)
      {
         struct sockaddr_in webaddr;
         bzero(&webaddr,sizeof(webaddr));
         webaddr.sin_family =AF_INET;
         webaddr.sin_port=htons(u->port)
      }

创建请求头模块:

      int


下载器模块

      int Cspider_Download(url*,const char*head,int webfd)
      {
         char buf[8192];
         char reshead[4096];
         int len;
         char * pos;
         bzero(buf,sizeof(buf));
         bzero(reshead,sizeof(reshead));

         send(webfd,head,strlen(head),0);
         printf("[5] Send Requesthead to web success..\n");

         len =recv(webfd,buf,"\r\n\r\n")
      }

      int Cspider_Create_Openssl()
      {  
         ssl_t *ssl
      }



      char aurl[4096];
      bzero(h1,siezof(h1));
      bzero(desc,sizeof(desc));
      close(fd);
      regex_t hreg,dreg,areg;
      regmatch_t hmatch[2],dmatch[2],amatch[3];
      regcomp(&hreg,"<h1>\\([^<]\\+\\?\\)</h1>",0);
      regcomp(&dreg,"<meta name="description" content="([^"]+?)">",0);